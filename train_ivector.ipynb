{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,sys\n",
    "sys.path.insert(0, './scripts')\n",
    "import ivector_dataset\n",
    "import siamese_model\n",
    "import ivector_tools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load i-vector feature from mgb dataset\n",
    "dataDir ='./mgbdata'\n",
    "languages = ['EGY','GLF','LAV','MSA','NOR']\n",
    "trn_labels = []\n",
    "trn_names = []\n",
    "trn_ivectors = np.empty((0,400))\n",
    "dev_labels = []\n",
    "dev_names = []\n",
    "dev_ivectors = np.empty((0,400))\n",
    "\n",
    "\n",
    "for i,lang in enumerate(languages):\n",
    "    #load train.vardial2017\n",
    "    filename = dataDir+'/train.vardial2017/%s.ivec' % lang\n",
    "    name   = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "    ivector = np.loadtxt(filename,usecols=range(1,401),dtype='float32')\n",
    "    trn_labels = np.append(trn_labels, np.ones(np.size(name))*(i+1))\n",
    "    trn_names=np.append(trn_names,name)\n",
    "    trn_ivectors = np.append(trn_ivectors, ivector,axis=0)\n",
    "\n",
    "    #load dev.vardial2017\n",
    "    filename = dataDir+'/dev.vardial2017/%s.ivec' % lang\n",
    "    name   = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "    ivector = np.loadtxt(filename,usecols=range(1,401),dtype='float32')\n",
    "    dev_names=np.append(dev_names,name)\n",
    "    dev_ivectors = np.append(dev_ivectors, ivector,axis=0)\n",
    "    dev_labels = np.append(dev_labels, np.ones(np.size(name))*(i+1))\n",
    "    \n",
    "# load test.MGB3\n",
    "filename = dataDir+'/test.MGB3/ivec_features'\n",
    "tst_name   = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ivectors = np.loadtxt(filename,usecols=range(1,401),dtype='float32')\n",
    "\n",
    "# merge trn+dev\n",
    "trndev_ivectors = np.append(trn_ivectors, dev_ivectors,axis=0)\n",
    "trndev_labels = np.append(trn_labels,dev_labels)\n",
    "trndev_name = np.append(trn_names,dev_names)\n",
    "\n",
    "# load tst.MGB3 labels\n",
    "filename = dataDir+'/test.MGB3/reference'\n",
    "tst_ref_name = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ref_label = np.loadtxt(filename,usecols=[1],dtype='int')\n",
    "\n",
    "tst_labels_index = []\n",
    "for i,name in enumerate(tst_name):\n",
    "    for j, name_ref in enumerate(tst_ref_name):\n",
    "        if name == name_ref:\n",
    "            tst_labels_index = np.append(tst_labels_index,j)\n",
    "\n",
    "tst_labels = np.empty((np.size(tst_labels_index)))\n",
    "for i,j in enumerate(tst_labels_index):\n",
    "    tst_labels[i]=tst_ref_label[int(j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((13825, 400), (1524, 400), (5, 400), (1492, 400))\n"
     ]
    }
   ],
   "source": [
    "#center and length norm. no.1\n",
    "m=np.mean(trn_ivectors,axis=0)\n",
    "A = np.cov(trn_ivectors.transpose())\n",
    "[a,D,V] = np.linalg.svd(A)\n",
    "V= V.transpose()\n",
    "W= np.dot(V, np.diag(1./( np.sqrt(D) + 0.0000000001 )))\n",
    "\n",
    "trn_ivectors = np.dot( np.subtract( trn_ivectors, m), W)\n",
    "trndev_ivectors = np.dot( np.subtract( trndev_ivectors, m), W)\n",
    "dev_ivectors = np.dot( np.subtract( dev_ivectors, m), W)\n",
    "tst_ivectors = np.dot( np.subtract( tst_ivectors, m), W)\n",
    "\n",
    "trn_ivectors = it.length_norm(trn_ivectors.transpose()).transpose()\n",
    "trndev_ivectors = it.length_norm(trndev_ivectors.transpose()).transpose()\n",
    "dev_ivectors = it.length_norm(dev_ivectors.transpose()).transpose()\n",
    "tst_ivectors = it.length_norm(tst_ivectors.transpose()).transpose()\n",
    "\n",
    "\n",
    "#language modeling\n",
    "lang_mean=[]\n",
    "for i, lang in enumerate(languages):\n",
    "#     lang_mean.append(np.mean( trn_ivectors[np.nonzero(trn_labels == i+1)][:],axis=0 ) )\n",
    "    lang_mean.append(np.mean(np.append(trndev_ivectors[np.nonzero(trndev_labels == i+1)] ,8*dev_ivectors[np.nonzero(dev_labels == i+1)],axis=0),axis=0))\n",
    "#     lang_mean.append(np.mean( trndev_ivectors[np.nonzero(trndev_labels == i+1)][:],axis=0 ) )\n",
    "\n",
    "lang_mean = np.array(lang_mean)\n",
    "lang_mean = it.length_norm(lang_mean.transpose()).transpose()\n",
    "\n",
    "print( np.shape(trn_ivectors), np.shape(dev_ivectors), np.shape(lang_mean),np.shape(tst_ivectors) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((69125,), (69125,), (69125,))\n",
      "((13825, 400), (13825, 400), (13825,), (55300, 400), (55300, 400), (55300,))\n"
     ]
    }
   ],
   "source": [
    "# making pair of train i-vector with mean of each language i-vector\n",
    "#  example : for total 3 ivectors\n",
    "#  ivector   ivector_p  label\n",
    "#     1         1         1\n",
    "#     1         2         0\n",
    "#     1         3         0\n",
    "#     2         1         0\n",
    "#     2         2         1\n",
    "#     ...      ...       ...\n",
    "#     3         3         1\n",
    "\n",
    "# preparing pair labels\n",
    "sim = []\n",
    "pair_a_idx = []\n",
    "pair_b_idx = []\n",
    "for i, lang in enumerate(languages):\n",
    "    for j, label in enumerate(trn_labels):\n",
    "#         print i, j, label\n",
    "        pair_a_idx.append(i+1)\n",
    "        pair_b_idx.append(j)\n",
    "        if i+1 == label:\n",
    "            sim.append(1)\n",
    "        else:\n",
    "            sim.append(0)\n",
    "print(np.shape(pair_a_idx),np.shape(pair_b_idx), np.shape(sim))\n",
    "pair_a_idx=np.array(pair_a_idx)\n",
    "pair_b_idx=np.array(pair_b_idx)\n",
    "sim = np.array(sim)\n",
    "\n",
    "#shuffling\n",
    "shuffleidx = np.arange(0,np.size(pair_a_idx))\n",
    "np.random.shuffle(shuffleidx)\n",
    "pair_a_idx = pair_a_idx[shuffleidx]\n",
    "pair_b_idx = pair_b_idx[shuffleidx]\n",
    "sim = sim[shuffleidx]\n",
    "\n",
    "\n",
    "data = []\n",
    "data_p = []\n",
    "    \n",
    "for iter in np.arange(0,np.size(sim)) :\n",
    "    data.append( lang_mean[pair_a_idx[iter]-1] )\n",
    "    data_p.append( trn_ivectors[pair_b_idx[iter]] )\n",
    "data = np.array(data)\n",
    "data_p = np.array(data_p)\n",
    "\n",
    "# TRN dataset loading for feeding \n",
    "tar_data = data[sim==1]\n",
    "tar_data_p = data_p[sim==1]\n",
    "tar_sim = sim[sim==1]\n",
    "non_data = data[sim==0]\n",
    "non_data_p = data_p[sim==0]\n",
    "non_sim = sim[sim==0]\n",
    "print(tar_data.shape, tar_data_p.shape,tar_sim.shape,non_data.shape,non_data_p.shape,non_sim.shape)\n",
    "\n",
    "trn_tar = ivector_dataset.DataSet(tar_data,tar_sim)\n",
    "trn_tar_p = ivector_dataset.DataSet(tar_data_p,tar_sim)\n",
    "\n",
    "trn_non = ivector_dataset.DataSet(non_data,non_sim)\n",
    "trn_non_p = ivector_dataset.DataSet(non_data_p,non_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((7620,), (7620,), (7620,))\n",
      "((1524, 400), (1524, 400), (1524,), (6096, 400), (6096, 400), (6096,))\n"
     ]
    }
   ],
   "source": [
    "# preparing pair labels of DEV dataset\n",
    "sim = []\n",
    "pair_a_idx = []\n",
    "pair_b_idx = []\n",
    "for i, lang in enumerate(languages):\n",
    "    for j, label in enumerate(dev_labels):\n",
    "#         print i, j, label\n",
    "        pair_a_idx.append(i+1)\n",
    "        pair_b_idx.append(j)\n",
    "        if i+1 == label:\n",
    "            sim.append(1)\n",
    "        else:\n",
    "            sim.append(0)\n",
    "print(np.shape(pair_a_idx),np.shape(pair_b_idx), np.shape(sim))\n",
    "pair_a_idx=np.array(pair_a_idx)\n",
    "pair_b_idx=np.array(pair_b_idx)\n",
    "sim = np.array(sim)\n",
    "\n",
    "#shuffling\n",
    "shuffleidx = np.arange(0,np.size(pair_a_idx))\n",
    "np.random.shuffle(shuffleidx)\n",
    "pair_a_idx = pair_a_idx[shuffleidx]\n",
    "pair_b_idx = pair_b_idx[shuffleidx]\n",
    "sim = sim[shuffleidx]\n",
    "\n",
    "\n",
    "data = []\n",
    "data_p = []\n",
    "    \n",
    "for iter in np.arange(0,np.size(sim)) :\n",
    "    data.append( lang_mean[pair_a_idx[iter]-1] )\n",
    "    data_p.append( dev_ivectors[pair_b_idx[iter]] )\n",
    "data = np.array(data)\n",
    "data_p = np.array(data_p)\n",
    "\n",
    "# DEV dataset loading for feeding \n",
    "tar_data = data[sim==1]\n",
    "tar_data_p = data_p[sim==1]\n",
    "tar_sim = sim[sim==1]\n",
    "non_data = data[sim==0]\n",
    "non_data_p = data_p[sim==0]\n",
    "non_sim = sim[sim==0]\n",
    "print(tar_data.shape, tar_data_p.shape,tar_sim.shape,non_data.shape,non_data_p.shape,non_sim.shape)\n",
    "\n",
    "dev_tar = ivector_dataset.DataSet(tar_data,tar_sim)\n",
    "dev_tar_p = ivector_dataset.DataSet(tar_data_p,tar_sim)\n",
    "\n",
    "dev_non = ivector_dataset.DataSet(non_data,non_sim)\n",
    "dev_non_p = ivector_dataset.DataSet(non_data_p,non_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16000)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "(?, 16000)\n",
      "(?, 1500)\n",
      "(?, 600)\n"
     ]
    }
   ],
   "source": [
    "# init variables\n",
    "sess = tf.InteractiveSession()\n",
    "siamese = siamese_model.siamese();\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.005, global_step,\n",
    "                                           5000, 0.99, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(siamese.loss, global_step=global_step)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6360: loss 0.958, Acc.: (DEV)0.438 (TST)0.408, lr : 0.00495\n",
      "Step 6440: loss 0.989, Acc.: (DEV)0.443 (TST)0.411, lr : 0.00495\n",
      "Step 6520: loss 0.965, Acc.: (DEV)0.446 (TST)0.421, lr : 0.00495\n",
      "Step 6620: loss 0.934, Acc.: (DEV)0.449 (TST)0.424, lr : 0.00495\n",
      "Step 6660: loss 0.931, Acc.: (DEV)0.432 (TST)0.426, lr : 0.00495\n",
      "Step 6740: loss 0.926, Acc.: (DEV)0.458 (TST)0.432, lr : 0.00495\n",
      "Step 6780: loss 0.964, Acc.: (DEV)0.500 (TST)0.438, lr : 0.00495\n",
      "Step 6860: loss 0.932, Acc.: (DEV)0.480 (TST)0.440, lr : 0.00495\n",
      "Step 6870: loss 0.946, Acc.: (DEV)0.495 (TST)0.443, lr : 0.00495\n",
      "Step 6910: loss 0.955, Acc.: (DEV)0.498 (TST)0.448, lr : 0.00495\n",
      "Step 7000: loss 0.922, Acc.: (DEV)0.503 (TST)0.448, lr : 0.00495\n",
      "Step 7010: loss 0.980, Acc.: (DEV)0.510 (TST)0.449, lr : 0.00495\n",
      "Step 7020: loss 1.008, Acc.: (DEV)0.488 (TST)0.456, lr : 0.00495\n",
      "Step 7040: loss 0.928, Acc.: (DEV)0.516 (TST)0.458, lr : 0.00495\n",
      "Step 7070: loss 0.923, Acc.: (DEV)0.518 (TST)0.459, lr : 0.00495\n",
      "Step 7090: loss 0.944, Acc.: (DEV)0.524 (TST)0.460, lr : 0.00495\n",
      "Step 7100: loss 0.955, Acc.: (DEV)0.535 (TST)0.464, lr : 0.00495\n",
      "Step 7110: loss 0.954, Acc.: (DEV)0.526 (TST)0.470, lr : 0.00495\n",
      "Step 7130: loss 0.939, Acc.: (DEV)0.543 (TST)0.478, lr : 0.00495\n",
      "Step 7170: loss 0.946, Acc.: (DEV)0.537 (TST)0.483, lr : 0.00495\n",
      "Step 7230: loss 0.923, Acc.: (DEV)0.547 (TST)0.486, lr : 0.00495\n",
      "Step 7250: loss 0.932, Acc.: (DEV)0.568 (TST)0.493, lr : 0.00495\n",
      "Step 7310: loss 0.939, Acc.: (DEV)0.549 (TST)0.494, lr : 0.00495\n",
      "Step 7320: loss 0.968, Acc.: (DEV)0.569 (TST)0.502, lr : 0.00495\n",
      "Step 7370: loss 0.933, Acc.: (DEV)0.576 (TST)0.506, lr : 0.00495\n",
      "Step 7400: loss 0.905, Acc.: (DEV)0.580 (TST)0.512, lr : 0.00495\n",
      "Step 7410: loss 0.960, Acc.: (DEV)0.589 (TST)0.517, lr : 0.00495\n",
      "Step 7420: loss 0.916, Acc.: (DEV)0.593 (TST)0.526, lr : 0.00495\n",
      "Step 7550: loss 0.905, Acc.: (DEV)0.603 (TST)0.543, lr : 0.00495\n",
      "Step 7610: loss 0.970, Acc.: (DEV)0.609 (TST)0.552, lr : 0.00495\n",
      "Step 7620: loss 0.919, Acc.: (DEV)0.610 (TST)0.555, lr : 0.00495\n",
      "Step 7770: loss 0.914, Acc.: (DEV)0.619 (TST)0.558, lr : 0.00495\n",
      "Step 7800: loss 0.935, Acc.: (DEV)0.623 (TST)0.564, lr : 0.00495\n",
      "Step 7830: loss 0.865, Acc.: (DEV)0.636 (TST)0.579, lr : 0.00495\n",
      "Step 7940: loss 0.885, Acc.: (DEV)0.650 (TST)0.582, lr : 0.00495\n",
      "Step 8050: loss 0.898, Acc.: (DEV)0.656 (TST)0.595, lr : 0.00495\n",
      "Step 8130: loss 0.828, Acc.: (DEV)0.661 (TST)0.607, lr : 0.00495\n",
      "Step 8240: loss 0.838, Acc.: (DEV)0.675 (TST)0.610, lr : 0.00495\n",
      "Step 8290: loss 0.875, Acc.: (DEV)0.693 (TST)0.625, lr : 0.00495\n",
      "Step 8350: loss 0.815, Acc.: (DEV)0.697 (TST)0.631, lr : 0.00495\n",
      "Step 8460: loss 0.834, Acc.: (DEV)0.699 (TST)0.634, lr : 0.00495\n",
      "Step 8470: loss 0.808, Acc.: (DEV)0.698 (TST)0.639, lr : 0.00495\n",
      "Step 8510: loss 0.888, Acc.: (DEV)0.712 (TST)0.644, lr : 0.00495\n",
      "Step 8520: loss 0.837, Acc.: (DEV)0.703 (TST)0.649, lr : 0.00495\n",
      "Step 8590: loss 0.914, Acc.: (DEV)0.715 (TST)0.651, lr : 0.00495\n",
      "Step 8610: loss 0.856, Acc.: (DEV)0.703 (TST)0.655, lr : 0.00495\n",
      "Step 8740: loss 0.833, Acc.: (DEV)0.730 (TST)0.660, lr : 0.00495\n",
      "Step 8900: loss 0.869, Acc.: (DEV)0.743 (TST)0.664, lr : 0.00495\n",
      "Step 9140: loss 0.789, Acc.: (DEV)0.757 (TST)0.670, lr : 0.00495\n",
      "Step 9330: loss 0.898, Acc.: (DEV)0.772 (TST)0.676, lr : 0.00495\n",
      "Step 9390: loss 0.793, Acc.: (DEV)0.776 (TST)0.680, lr : 0.00495\n",
      "Step 9590: loss 0.802, Acc.: (DEV)0.798 (TST)0.680, lr : 0.00495\n",
      "Step 9600: loss 0.825, Acc.: (DEV)0.797 (TST)0.683, lr : 0.00495\n",
      "Step 9800: loss 0.770, Acc.: (DEV)0.808 (TST)0.685, lr : 0.00495\n",
      "Step 9830: loss 0.802, Acc.: (DEV)0.809 (TST)0.691, lr : 0.00495\n",
      "Step 10000: loss 0.791, Acc.: (DEV)0.816 (TST)0.694, lr : 0.00490\n",
      "Step 10010: loss 0.807, Acc.: (DEV)0.818 (TST)0.699, lr : 0.00490\n",
      "Step 10220: loss 0.759, Acc.: (DEV)0.824 (TST)0.702, lr : 0.00490\n",
      "Step 10350: loss 0.908, Acc.: (DEV)0.837 (TST)0.705, lr : 0.00490\n",
      "Step 10470: loss 0.764, Acc.: (DEV)0.842 (TST)0.708, lr : 0.00490\n",
      "Step 10550: loss 0.749, Acc.: (DEV)0.839 (TST)0.708, lr : 0.00490\n",
      "Step 10610: loss 0.830, Acc.: (DEV)0.848 (TST)0.710, lr : 0.00490\n",
      "Step 10920: loss 0.723, Acc.: (DEV)0.848 (TST)0.711, lr : 0.00490\n",
      "Step 11420: loss 0.666, Acc.: (DEV)0.884 (TST)0.712, lr : 0.00490\n",
      "Step 11430: loss 0.699, Acc.: (DEV)0.885 (TST)0.713, lr : 0.00490\n",
      "Step 11460: loss 0.688, Acc.: (DEV)0.884 (TST)0.714, lr : 0.00490\n",
      "Step 11670: loss 0.731, Acc.: (DEV)0.892 (TST)0.714, lr : 0.00490\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "batch_size = 50\n",
    "max_acc = 0.40\n",
    "max_step=0\n",
    "saver_folder='snnmodel_ivector'\n",
    "if not os.path.exists(saver_folder):\n",
    "    os.mkdir(saver_folder)\n",
    "for step in range(100000):\n",
    "  \n",
    "    if step %5 ==0:\n",
    "        batch_x1_a, batch_y1_a = trn_tar.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_a, batch_y2_a = trn_tar_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1_b, batch_y1_b = trn_non.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_b, batch_y2_b = trn_non_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1 = np.append(batch_x1_a,batch_x1_b,axis=0)\n",
    "        batch_y1 = np.append(batch_y1_a,batch_y1_b,axis=0)\n",
    "        batch_x2 = np.append(batch_x2_a,batch_x2_b,axis=0)\n",
    "        batch_y2 = np.append(batch_y2_a,batch_y2_b,axis=0)\n",
    "    else:\n",
    "        batch_x1_a, batch_y1_a = dev_tar.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_a, batch_y2_a = dev_tar_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1_b, batch_y1_b = dev_non.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_b, batch_y2_b = dev_non_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1 = np.append(batch_x1_a,batch_x1_b,axis=0)\n",
    "        batch_y1 = np.append(batch_y1_a,batch_y1_b,axis=0)\n",
    "        batch_x2 = np.append(batch_x2_a,batch_x2_b,axis=0)\n",
    "        batch_y2 = np.append(batch_y2_a,batch_y2_b,axis=0)\n",
    "        \n",
    "\n",
    "#     batch_x1,batch_y1 = mgb3_siam1.train.next_batch(120,shuffle=False)\n",
    "#     batch_x2,batch_y2 = mgb3_siam2.train.next_batch(120,shuffle=False)    \n",
    "#     batch_y = (batch_y1==batch_y2).astype('float')\n",
    "    batch_y = batch_y1*2-1\n",
    "#     batch_y = 1-batch_y1\n",
    "    \n",
    "    _, loss_v = sess.run([train_step, siamese.loss], feed_dict={\n",
    "        siamese.x1: batch_x1,\n",
    "        siamese.x2: batch_x2,\n",
    "        siamese.y_: batch_y\n",
    "    })\n",
    "    \n",
    "    if np.isnan(loss_v):\n",
    "        print ('Model diverged with loss = NAN')\n",
    "        quit()\n",
    "        \n",
    "    if step % 10 ==0:\n",
    "        dev_ivectors_siam = siamese.o1.eval({siamese.x1:dev_ivectors})\n",
    "        lang_mean_siam = siamese.o1.eval({siamese.x1:lang_mean})\n",
    "        tst_ivectors_siam = siamese.o1.eval({siamese.x1:tst_ivectors})\n",
    "#         dev_scores=[]\n",
    "#         for iter in range(5):\n",
    "#             dev_scores.append( np.sum(np.power(lang_mean_siam[iter]-dev_ivectors_siam,2),1) )\n",
    "#         dev_scores = np.array(dev_scores)\n",
    "        dev_scores = lang_mean_siam.dot(dev_ivectors_siam.transpose() )\n",
    "        hypo_lang = np.argmax(dev_scores,axis = 0)\n",
    "        temp = ((dev_labels-1) - hypo_lang)\n",
    "        acc =1- np.size(np.nonzero(temp)) / float( np.size(dev_labels) )\n",
    "        \n",
    "        tst_scores = lang_mean_siam.dot(tst_ivectors_siam.transpose() )\n",
    "        hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "        temp = ((tst_labels-1) - hypo_lang)\n",
    "        acc_tst =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "\n",
    "        if max_acc < acc_tst:\n",
    "            max_acc = acc_tst\n",
    "            max_step=step\n",
    "            print ('Step %d: loss %.3f, Acc.: (DEV)%.3f (TST)%.3f, lr : %.5f' % (step,loss_v,acc,acc_tst,sess.run(learning_rate)))\n",
    "            saver.save(sess, saver_folder+'/model'+str(step)+'.ckpt')\n",
    "        if loss_v<0.3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print max_step\n",
    "RESTORE_STEP=max_step\n",
    "saver.restore(sess, saver_folder+'/model'+str(RESTORE_STEP)+'.ckpt')\n",
    "\n",
    "\n",
    "trn_ivectors_siam = siamese.o1.eval({siamese.x1:trn_ivectors})\n",
    "dev_ivectors_siam = siamese.o1.eval({siamese.x1:dev_ivectors})\n",
    "tst_ivectors_siam = siamese.o1.eval({siamese.x1:tst_ivectors})\n",
    "lang_mean_siam = siamese.o1.eval({siamese.x1:lang_mean})\n",
    "\n",
    "tst_scores = lang_mean_siam.dot(tst_ivectors_siam.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "print 'Final accurary on test dataset : %0.3f' %(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmat = np.zeros((5,5))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ tst_labels == i+1]\n",
    "    hist_bylang = np.histogram(hypo_bylang,5)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
