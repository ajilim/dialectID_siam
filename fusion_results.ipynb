{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,sys\n",
    "sys.path.insert(0, './scripts')\n",
    "dataDir ='./data'\n",
    "\n",
    "\n",
    "import py_compile\n",
    "py_compile.compile('scripts/ivector_dataset.py')\n",
    "py_compile.compile('scripts/siamese_model.py')\n",
    "import ivector_dataset\n",
    "import siamese_model\n",
    "import ivector_tools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_size(dict_file,feat_file):\n",
    "# Counting feature dimension and total number of utterances\n",
    "    f = open(dict_file)\n",
    "    dict_dim = 0\n",
    "    for line in f:\n",
    "        dict_dim+=1\n",
    "    f.close()\n",
    "    feat_len = 0\n",
    "    f = open(feat_file)\n",
    "    for line in f:\n",
    "        feat_len+=1\n",
    "    f.close()\n",
    "    return dict_dim, feat_len\n",
    "\n",
    "def get_feat_label(dict_file, feat_file):\n",
    "# Get feature vectors from files\n",
    "    dict_dim, feat_len = get_dataset_size(dict_file,feat_file)\n",
    "    features = np.zeros((feat_len,dict_dim),dtype='float32')\n",
    "    labels = np.zeros((feat_len),dtype='int8')\n",
    "    names = []\n",
    "    f = open(feat_file)\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        names.append(line.split()[0])\n",
    "        labels[count] = line.split()[1]\n",
    "        line= line.split()[2:]\n",
    "        for iter in range(0,len(line)):\n",
    "            elements = line[iter].split(':')\n",
    "            features[count][ int( elements[0] ) -1 ] = elements[1]\n",
    "        count = count + 1 \n",
    "    f.close()\n",
    "    \n",
    "    return features, labels, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((13825, 400), (1524, 400), (5, 400), (1492, 400))\n",
      "Baseline accurary on test dataset : 0.668\n"
     ]
    }
   ],
   "source": [
    "# write prototxt for siamese network\n",
    "\n",
    "languages = ['EGY','GLF','LAV','MSA','NOR']\n",
    "trn_labels = []\n",
    "trn_names = []\n",
    "trn_ivectors = np.empty((0,400))\n",
    "dev_labels = []\n",
    "dev_names = []\n",
    "dev_ivectors = np.empty((0,400))\n",
    "\n",
    "\n",
    "for i,lang in enumerate(languages):\n",
    "    #load train.vardial2017\n",
    "    filename = dataDir+'/train.vardial2017/%s.ivec' % lang\n",
    "    name   = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "    ivector = np.loadtxt(filename,usecols=range(1,401),dtype='float32')\n",
    "    trn_labels = np.append(trn_labels, np.ones(np.size(name))*(i+1))\n",
    "    trn_names=np.append(trn_names,name)\n",
    "    trn_ivectors = np.append(trn_ivectors, ivector,axis=0)\n",
    "\n",
    "    #load dev.vardial2017\n",
    "    filename = dataDir+'/dev.vardial2017/%s.ivec' % lang\n",
    "    name   = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "    ivector = np.loadtxt(filename,usecols=range(1,401),dtype='float32')\n",
    "    dev_names=np.append(dev_names,name)\n",
    "    dev_ivectors = np.append(dev_ivectors, ivector,axis=0)\n",
    "    dev_labels = np.append(dev_labels, np.ones(np.size(name))*(i+1))\n",
    "    \n",
    "# load test.MGB3\n",
    "filename = dataDir+'/test.MGB3/ivec_features'\n",
    "tst_names   = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ivectors = np.loadtxt(filename,usecols=range(1,401),dtype='float32')\n",
    "\n",
    "# merge trn+dev\n",
    "trndev_ivectors = np.append(trn_ivectors, dev_ivectors,axis=0)\n",
    "trndev_labels = np.append(trn_labels,dev_labels)\n",
    "trndev_name = np.append(trn_names,dev_names)\n",
    "\n",
    "\n",
    "# load tst.MGB3 labels\n",
    "filename = 'data/test.MGB3/reference'\n",
    "tst_ref_names = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ref_labels = np.loadtxt(filename,usecols=[1],dtype='int')\n",
    "\n",
    "tst_ref_labels_index = []\n",
    "for i, name_ref in enumerate(tst_ref_names):\n",
    "    for j, name in enumerate(tst_names):\n",
    "        if name == name_ref:\n",
    "            tst_ref_labels_index = np.append(tst_ref_labels_index,int(j))\n",
    "\n",
    "tst_labels = tst_ref_labels\n",
    "tst_ivectors = tst_ivectors[ map(int,tst_ref_labels_index),:]\n",
    "\n",
    "#center and length norm.\n",
    "m=np.mean(trn_ivectors,axis=0)\n",
    "A = np.cov(trn_ivectors.transpose())\n",
    "[a,D,V] = np.linalg.svd(A)\n",
    "V= V.transpose()\n",
    "W= np.dot(V, np.diag(1./( np.sqrt(D) + 0.0000000001 )))\n",
    "\n",
    "trn_ivectors = np.dot( np.subtract( trn_ivectors, m), W)\n",
    "trndev_ivectors = np.dot( np.subtract( trndev_ivectors, m), W)\n",
    "dev_ivectors = np.dot( np.subtract( dev_ivectors, m), W)\n",
    "tst_ivectors = np.dot( np.subtract( tst_ivectors, m), W)\n",
    "\n",
    "trn_ivectors = it.length_norm(trn_ivectors)\n",
    "trndev_ivectors = it.length_norm(trndev_ivectors)\n",
    "dev_ivectors = it.length_norm(dev_ivectors)\n",
    "tst_ivectors = it.length_norm(tst_ivectors)\n",
    "\n",
    "\n",
    "#language modeling\n",
    "lang_mean=[]\n",
    "for i, lang in enumerate(languages):\n",
    "    lang_mean.append(np.mean(np.append(trn_ivectors[np.nonzero(trn_labels == i+1)] ,9*dev_ivectors[np.nonzero(dev_labels == i+1)],axis=0),axis=0))\n",
    "#     lang_mean.append(np.mean(dev_ivectors[np.nonzero(dev_labels == i+1)],axis=0))\n",
    "\n",
    "lang_mean = np.array(lang_mean)\n",
    "lang_mean = it.length_norm(lang_mean)\n",
    "\n",
    "print( np.shape(trn_ivectors), np.shape(dev_ivectors), np.shape(lang_mean),np.shape(tst_ivectors) )\n",
    "\n",
    "\n",
    "# Baseline performance on TST using CDS\n",
    "tst_scores = lang_mean.dot(tst_ivectors.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "print 'Baseline accurary on test dataset : %0.3f' %(acc)\n",
    "\n",
    "tst_scores_ivector_baseline = tst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16000)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "(?, 16000)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "42320\n",
      "INFO:tensorflow:Restoring parameters from snnmodel_ivector_backup/model42320.ckpt\n",
      "Final accurary on test dataset : 0.731\n",
      "Confusion matrix\n",
      "[[ 225.   13.   37.   11.   24.]\n",
      " [  11.  178.   31.   16.   12.]\n",
      " [  43.   45.  212.   14.   31.]\n",
      " [   5.    5.   10.  208.   10.]\n",
      " [  18.    9.   44.   13.  267.]]\n",
      "Precision\n",
      "[ 0.72580645  0.71774194  0.61449275  0.87394958  0.76068376]\n",
      "Recall\n",
      "[ 0.74503311  0.712       0.63473054  0.79389313  0.77616279]\n",
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.731\n",
      "Precision : 0.739\n",
      "Recall    : 0.732\n"
     ]
    }
   ],
   "source": [
    "# init variables\n",
    "sess = tf.InteractiveSession()\n",
    "siamese = siamese_model.siamese();\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.005, global_step,\n",
    "                                           5000, 0.99, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(siamese.loss, global_step=global_step)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "saver_folder='snnmodel_ivector_backup'\n",
    "# saver_folder='snnmodel_ivector'\n",
    "\n",
    "max_step = 42320\n",
    "# max_step = 51240\n",
    "\n",
    "print max_step\n",
    "RESTORE_STEP=max_step\n",
    "saver.restore(sess, saver_folder+'/model'+str(RESTORE_STEP)+'.ckpt')\n",
    "\n",
    "\n",
    "# trn_ivectors_siam = siamese.o1.eval({siamese.x1:trn_ivectors})\n",
    "dev_ivectors_siam = siamese.o1.eval({siamese.x1:dev_ivectors})\n",
    "tst_ivectors_siam = siamese.o1.eval({siamese.x1:tst_ivectors})\n",
    "lang_mean_siam = siamese.o1.eval({siamese.x1:lang_mean})\n",
    "\n",
    "tst_scores = lang_mean_siam.dot(tst_ivectors_siam.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "print 'Final accurary on test dataset : %0.3f' %(acc)\n",
    "\n",
    "confusionmat = np.zeros((5,5))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ tst_labels == i+1]\n",
    "    hist_bylang = np.histogram(hypo_bylang,5)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n",
    "\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "tst_scores_ivectors = tst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 41657) (1524, 41657) (1492, 41657)\n",
      "((14000, 41657), (1524, 41657), (5, 41657), (1492, 41657))\n"
     ]
    }
   ],
   "source": [
    "context = 1\n",
    "dict_file = 'data/train.vardial2017/dict.words.c'+str(context)\n",
    "feat_file = 'data/train.vardial2017/words.c'+str(context)\n",
    "trn_features, trn_labels, trn_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "feat_file = 'data/dev.vardial2017/words.c'+str(context)\n",
    "dev_features, dev_labels, dev_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "feat_file = 'data/test.MGB3/words.c'+str(context)\n",
    "tst_features, tst_labels, tst_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "print trn_features.shape, dev_features.shape, tst_features.shape\n",
    "\n",
    "languages = ['EGY','GLF','LAV','MSA','NOR']\n",
    "\n",
    "# load tst.MGB3 labels\n",
    "filename = 'data/test.MGB3/reference'\n",
    "tst_ref_names = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ref_labels = np.loadtxt(filename,usecols=[1],dtype='int')\n",
    "\n",
    "tst_ref_labels_index = []\n",
    "for i, name_ref in enumerate(tst_ref_names):\n",
    "    for j, name in enumerate(tst_names):\n",
    "        if name == name_ref:\n",
    "            tst_ref_labels_index = np.append(tst_ref_labels_index,int(j))\n",
    "\n",
    "tst_labels = tst_ref_labels\n",
    "tst_features = tst_features[ map(int,tst_ref_labels_index),:]\n",
    "    \n",
    "#language modeling\n",
    "lang_mean=[]\n",
    "for i, lang in enumerate(languages):\n",
    "   lang_mean.append(np.mean( trn_features[np.nonzero(trn_labels == i+1)][:],axis=0 ) )\n",
    "#     lang_mean.append(np.mean(np.append(trn_features[np.nonzero(trn_labels == i+1)] ,9*dev_features[np.nonzero(dev_labels == i+1)],axis=0),axis=0))\n",
    "\n",
    "lang_mean = np.array(lang_mean)\n",
    "lang_mean = it.length_norm(lang_mean)\n",
    "\n",
    "print( np.shape(trn_features), np.shape(dev_features), np.shape(lang_mean),np.shape(tst_features) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 92600)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "(?, 92600)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "60400\n",
      "INFO:tensorflow:Restoring parameters from snnmodel_words/model60400.ckpt\n",
      "Final accurary on test dataset : 0.585\n",
      "Confusion matrix\n",
      "[[ 179.   31.   38.   15.   29.]\n",
      " [  24.  119.   44.   28.   56.]\n",
      " [  54.   41.  208.   23.   47.]\n",
      " [  16.   31.   17.  184.   29.]\n",
      " [  29.   28.   27.   12.  183.]]\n",
      "Precision\n",
      "[ 0.6130137   0.43911439  0.55764075  0.66425993  0.65591398]\n",
      "Recall\n",
      "[ 0.59271523  0.476       0.62275449  0.70229008  0.53197674]\n",
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.585\n",
      "Precision : 0.586\n",
      "Recall    : 0.585\n"
     ]
    }
   ],
   "source": [
    "import siamese_model_words as siamese_model\n",
    "\n",
    "# init variables\n",
    "sess = tf.InteractiveSession()\n",
    "siamese = siamese_model.siamese(np.shape(trn_features)[1]);\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step,\n",
    "                                           5000, 0.99, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(siamese.loss, global_step=global_step)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver_folder='snnmodel_words'\n",
    "max_step = 60400\n",
    "print max_step\n",
    "RESTORE_STEP=max_step\n",
    "saver.restore(sess, saver_folder+'/model'+str(RESTORE_STEP)+'.ckpt')\n",
    "\n",
    "\n",
    "# trn_features_siam = siamese.o1.eval({siamese.x1:trn_features})\n",
    "# dev_features_siam = siamese.o1.eval({siamese.x1:dev_features})\n",
    "tst_features_siam = siamese.o1.eval({siamese.x1:tst_features})\n",
    "lang_mean_siam = siamese.o1.eval({siamese.x1:lang_mean})\n",
    "\n",
    "tst_scores = lang_mean_siam.dot(tst_features_siam.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "print 'Final accurary on test dataset : %0.3f' %(acc)\n",
    "\n",
    "confusionmat = np.zeros((5,5))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ tst_labels == i+1]\n",
    "    hist_bylang = np.histogram(hypo_bylang,5)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n",
    "\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "tst_scores_words = tst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 13285) (1524, 13285) (1492, 13285)\n",
      "((14000, 13285), (1524, 13285), (5, 13285), (1492, 13285))\n"
     ]
    }
   ],
   "source": [
    "import siamese_model_chars as siamese_model\n",
    "\n",
    "context = 3\n",
    "dict_file = 'data/train.vardial2017/dict.chars.c'+str(context)\n",
    "feat_file = 'data/train.vardial2017/chars.c'+str(context)\n",
    "trn_features, trn_labels, trn_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "feat_file = 'data/dev.vardial2017/chars.c'+str(context)\n",
    "dev_features, dev_labels, dev_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "feat_file = 'data/test.MGB3/chars.c'+str(context)\n",
    "tst_features, tst_labels, tst_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "print trn_features.shape, dev_features.shape, tst_features.shape\n",
    "\n",
    "languages = ['EGY','GLF','LAV','MSA','NOR']\n",
    "\n",
    "# load tst.MGB3 labels\n",
    "filename = 'data/test.MGB3/reference'\n",
    "tst_ref_names = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ref_labels = np.loadtxt(filename,usecols=[1],dtype='int')\n",
    "\n",
    "tst_ref_labels_index = []\n",
    "for i, name_ref in enumerate(tst_ref_names):\n",
    "    for j, name in enumerate(tst_names):\n",
    "        if name == name_ref:\n",
    "            tst_ref_labels_index = np.append(tst_ref_labels_index,int(j))\n",
    "\n",
    "tst_labels = tst_ref_labels\n",
    "tst_features = tst_features[ map(int,tst_ref_labels_index),:]\n",
    "\n",
    "\n",
    "#language modeling\n",
    "lang_mean=[]\n",
    "for i, lang in enumerate(languages):\n",
    "    lang_mean.append(np.mean( trn_features[np.nonzero(trn_labels == i+1)][:],axis=0 ) )\n",
    "\n",
    "lang_mean = np.array(lang_mean)\n",
    "lang_mean = it.length_norm(lang_mean)\n",
    "\n",
    "print( np.shape(trn_features), np.shape(dev_features), np.shape(lang_mean),np.shape(tst_features) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 88600)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "(?, 88600)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "71600\n",
      "INFO:tensorflow:Restoring parameters from snnmodel_chars/model71600.ckpt\n",
      "Final accurary on test dataset : 0.582\n",
      "Confusion matrix\n",
      "[[ 170.   32.   32.   11.   34.]\n",
      " [  30.  108.   30.   24.   35.]\n",
      " [  57.   44.  214.   24.   50.]\n",
      " [  17.   32.   16.  175.   24.]\n",
      " [  28.   34.   42.   28.  201.]]\n",
      "Precision\n",
      "[ 0.609319    0.47577093  0.55012853  0.66287879  0.6036036 ]\n",
      "Recall\n",
      "[ 0.56291391  0.432       0.64071856  0.66793893  0.58430233]\n",
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.582\n",
      "Precision : 0.580\n",
      "Recall    : 0.578\n"
     ]
    }
   ],
   "source": [
    "# init variables\n",
    "sess = tf.InteractiveSession()\n",
    "siamese = siamese_model.siamese(np.shape(trn_features)[1]);\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step,\n",
    "                                           5000, 0.99, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(siamese.loss, global_step=global_step)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver_folder='snnmodel_chars'\n",
    "\n",
    "max_step = 71600\n",
    "print max_step\n",
    "RESTORE_STEP=max_step\n",
    "saver.restore(sess, saver_folder+'/model'+str(RESTORE_STEP)+'.ckpt')\n",
    "\n",
    "\n",
    "# trn_features_siam = siamese.o1.eval({siamese.x1:trn_features})\n",
    "# dev_features_siam = siamese.o1.eval({siamese.x1:dev_features})\n",
    "tst_features_siam = siamese.o1.eval({siamese.x1:tst_features})\n",
    "lang_mean_siam = siamese.o1.eval({siamese.x1:lang_mean})\n",
    "\n",
    "tst_scores = lang_mean_siam.dot(tst_features_siam.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "print 'Final accurary on test dataset : %0.3f' %(acc)\n",
    "\n",
    "confusionmat = np.zeros((5,5))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ tst_labels == i+1]\n",
    "    hist_bylang = np.histogram(hypo_bylang,5)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n",
    "\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "tst_scores_chars = tst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13825, 50320) (1524, 50320) (1492, 50320)\n",
      "((13825, 50320), (1524, 50320), (5, 50320), (1492, 50320))\n"
     ]
    }
   ],
   "source": [
    "import siamese_model_phone_hu as siamese_model\n",
    "\n",
    "context = 3\n",
    "dict_file = 'data/train.vardial2017/dict.phone_hu.c'+str(context)\n",
    "feat_file = 'data/train.vardial2017/phone_hu.c'+str(context)\n",
    "trn_features, trn_labels, trn_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "feat_file = 'data/dev.vardial2017/phone_hu.c'+str(context)\n",
    "dev_features, dev_labels, dev_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "feat_file = 'data/test.MGB3/phone_hu.c'+str(context)\n",
    "tst_features, tst_labels, tst_names = get_feat_label(dict_file,feat_file)\n",
    "\n",
    "print trn_features.shape, dev_features.shape, tst_features.shape\n",
    "\n",
    "languages = ['EGY','GLF','LAV','MSA','NOR']\n",
    "\n",
    "# load tst.MGB3 labels\n",
    "filename = 'data/test.MGB3/reference'\n",
    "tst_ref_names = np.loadtxt(filename,usecols=[0],dtype='string')\n",
    "tst_ref_labels = np.loadtxt(filename,usecols=[1],dtype='int')\n",
    "\n",
    "tst_ref_labels_index = []\n",
    "for i, name_ref in enumerate(tst_ref_names):\n",
    "    for j, name in enumerate(tst_names):\n",
    "        if name == name_ref:\n",
    "            tst_ref_labels_index = np.append(tst_ref_labels_index,int(j))\n",
    "\n",
    "tst_labels = tst_ref_labels\n",
    "tst_features = tst_features[ map(int,tst_ref_labels_index),:]\n",
    "    \n",
    "    \n",
    "#language modeling\n",
    "lang_mean=[]\n",
    "for i, lang in enumerate(languages):\n",
    "#     lang_mean.append(np.mean(np.append(trn_features[np.nonzero(trn_labels == i+1)] ,dev_features[np.nonzero(dev_labels == i+1)],axis=0),axis=0))\n",
    "    lang_mean.append(np.mean( trn_features[np.nonzero(trn_labels == i+1)][:],axis=0 ) )\n",
    "\n",
    "lang_mean = np.array(lang_mean)\n",
    "lang_mean = it.length_norm(lang_mean)\n",
    "\n",
    "print( np.shape(trn_features), np.shape(dev_features), np.shape(lang_mean),np.shape(tst_features) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 91520)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "(?, 91520)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "60400\n",
      "INFO:tensorflow:Restoring parameters from snnmodel_phone_hu_backup/model60400.ckpt\n",
      "Final accurary on test dataset : 0.548\n",
      "Confusion matrix\n",
      "[[ 170.   34.   73.   19.   50.]\n",
      " [  25.  132.   62.   21.   27.]\n",
      " [  46.   55.  137.   19.   51.]\n",
      " [  17.   11.   15.  187.   25.]\n",
      " [  44.   18.   47.   16.  191.]]\n",
      "Precision\n",
      "[ 0.49132948  0.49438202  0.44480519  0.73333333  0.60443038]\n",
      "Recall\n",
      "[ 0.56291391  0.528       0.41017964  0.71374046  0.55523256]\n",
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.548\n",
      "Precision : 0.554\n",
      "Recall    : 0.554\n"
     ]
    }
   ],
   "source": [
    "# init variables\n",
    "sess = tf.InteractiveSession()\n",
    "siamese = siamese_model.siamese(np.shape(trn_features)[1]);\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step,\n",
    "                                           5000, 0.99, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(siamese.loss, global_step=global_step)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver_folder='snnmodel_phone_hu_backup'\n",
    "\n",
    "max_step = 60400\n",
    "print max_step\n",
    "RESTORE_STEP=max_step\n",
    "saver.restore(sess, saver_folder+'/model'+str(RESTORE_STEP)+'.ckpt')\n",
    "\n",
    "\n",
    "# trn_features_siam = siamese.o1.eval({siamese.x1:trn_features})\n",
    "# dev_features_siam = siamese.o1.eval({siamese.x1:dev_features})\n",
    "tst_features_siam = siamese.o1.eval({siamese.x1:tst_features})\n",
    "lang_mean_siam = siamese.o1.eval({siamese.x1:lang_mean})\n",
    "\n",
    "tst_scores = lang_mean_siam.dot(tst_features_siam.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "print 'Final accurary on test dataset : %0.3f' %(acc)\n",
    "\n",
    "confusionmat = np.zeros((5,5))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ tst_labels == i+1]\n",
    "    hist_bylang = np.histogram(hypo_bylang,5)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n",
    "\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "tst_scores_phone_hu = tst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.755\n",
      "Precision : 0.554\n",
      "Recall    : 0.554\n"
     ]
    }
   ],
   "source": [
    "# Fusion 1 : ivector + chars\n",
    "\n",
    "tst_scores = tst_scores_ivectors + tst_scores_chars\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.751\n",
      "Precision : 0.554\n",
      "Recall    : 0.554\n"
     ]
    }
   ],
   "source": [
    "# Fusion 2 : ivector + words\n",
    "\n",
    "tst_scores = tst_scores_ivectors + tst_scores_words\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.716\n",
      "Precision : 0.554\n",
      "Recall    : 0.554\n"
     ]
    }
   ],
   "source": [
    "# Fusion 3 : ivector + phone_hu\n",
    "\n",
    "tst_scores = tst_scores_ivectors + tst_scores_phone_hu\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.752\n",
      "Precision : 0.554\n",
      "Recall    : 0.554\n"
     ]
    }
   ],
   "source": [
    "# Fusion 4 : All\n",
    "\n",
    "tst_scores = 1*tst_scores_ivectors + 1*tst_scores_words + 1*tst_scores_chars + 1*tst_scores_phone_hu\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.775\n",
      "Precision : 0.554\n",
      "Recall    : 0.554\n"
     ]
    }
   ],
   "source": [
    "# Fusion - : All + linear combination\n",
    "\n",
    "tst_scores = 2.5*tst_scores_ivectors + 1*tst_scores_words + 1*tst_scores_chars + 1*tst_scores_phone_hu\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "temp = ((tst_labels-1) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(tst_labels))\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
